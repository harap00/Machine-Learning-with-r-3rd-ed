---
title: "6_Predicting numerical data regression methods"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

# 第６章 数値データを予測する―回帰法

　本章では数値データ間の関係を予測するための手法を学ぶ。内容は次の通り。

-   回帰で使われている基本的な統計学の原理
-   回帰分類のためのデータの前処理と、回帰モデルの推定と解釈
-   回帰木及びモデル木と呼ばれるハイブリッド手法

```{r}
# ライブラリの読み込み
if(! require(tidyverse)){
    install.packages('ridyverse', quiet = TRUE)
}

library(tidyverse)
```

## 6.1 回帰を理解する

　回帰の目的は、従属変数（dependent variable）と独立変数（independent variable）の関係を明らかにすることである。

　回帰方程式は、傾き切片形と同じような直線を使ってデータをモデル化する。回帰分析は、過去の事象をモデル化することと、そのモデルを使って未来の事象を予測することについて利用できる。

　具体的なユースケースをいくつかあげる。

-   観測された属性に基づく集団や個体のばらつきの調査。この分析は、経済学、社会学、心理学、物理学、生態学の分野での科学的調査に利用される
-   事象と反応の間の因果関係の定量化。医薬品の治験、高額の安全性検査、市場調査などに利用される。
-   基地の基準に照らして未来の挙動を予測するために利用できるパターンの特定。保険請求、自然災害の被害、選挙結果、犯罪率の予測に利用される。

### 6.1.1 単回帰

　単回帰モデルは、従属変数と単一の独立変数との関係を定義する。この定義には、次の式によって表される直線を使う。 　$$y = \alpha + \beta x$$

　この式は傾き切片形の式と同じである。切片$\alpha$は直線が$ｙ$軸と交差する点を表し、傾き$\beta$は$x$の値が増えるたびに$y$がどのくらい変化するのかを表す。

| 上式の$\alpha$と$\beta$の推定値は一般に$a$と$b$を使って記述する。

### 6.1.2 最小二乗法による推定

　$\alpha$と$\beta$の最適な推定値の特定には、**最小二乗法**という推定法を使った。最小二乗回帰の目的は、次の式を最小化することとして表現される。

$$
\sum{(y_i - \hat{y_i})^2} = \sum{e_i^2}
$$

　また、$a$の解は$b$の値によって決まる。$a$の値を求める式は次のようになる。

$$
a=\bar{y}-b\bar{x}
$$

　検証用のデータとして、スペースシャトルの打ち上げデータを使用する。

```{r}
# ファイルの読み込み
launch <- read.csv('https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(3rd%20Ed.)/Chapter06/challenger.csv', stringsAsFactors = TRUE)

launch %>% head(n = 3)
```

　上記データを使い独立変数 temparature 従属変数が distress_ct とすると傾き$b$の値はRの関数covとvarを使って推定できる。

```{r}
b <- cov(launch$temperature, launch$distress_ct) / var(launch$temperature)
b
```

　そして、この$b$の値とmean関数を使って$a$の値を推定できる。

```{r}
a <- mean(launch$distress_ct) - b * mean(launch$temperature)
a
```

### 6.1.3 相関

　2つの変数の間の**相関**（correlation）は、それらの関係がどれくらい直線的であるかを数値化したものである。修飾子が付かない単なる「相関」は、一般に、**ピアソンの相関係数**（Pearson correlation coefficient）を指している。

　ピアソンの相関係数は次の式によって定義される。

$$
\rho_{x,y}=Corr(x,y)=\frac{Cov(x,y)}{\rho_x\rho_y}
$$

　この式を使って、スペースシャトルの打ち上げ時の気温と破損したOリングの数との相関を計算できる。

```{r}
r <- cov(launch$temperature, launch$distress_ct) / (sd(launch$temperature) * sd(launch$distress_ct))

r
```

　相関関数corでも同様の結果が得られる

```{r}
cor(launch$temperature, launch$distress_ct)
```

　このデータにおけるスペースシャトルの打ち上げ時の気温と破損したOリングの一人の相関は-0.51となった。これは比較的強い負の線形連関があることを示している。

　相関の強さを解釈するための目安は様々ある。例えば、0.1～0.3の値を「弱い」、0.3～0.5の値を「中くらい」、0.5を超える値を「強い」と判断するルールもある（負の相関の範囲も同様）。しかし、目的によっては、これらの閾値が[厳しすぎる]{.underline}または[緩すぎる]{.underline}こともある。多くの場合、相関はコンテキストの中で解釈する必要がある。人間が関わっているデータでは、0.5の相関が極めて高いと見なされるとしても、機械的なプロセスで生成したデータでは、0.5の相関は非常に弱いかもしれない。

> 【Tips】
>
> 　「相関は因果関係を含意しない」という表現を聞いたことがあるかもしれない。この表現の根底にあるのは、相関によって表されるのは２つの変数間の関連性だけで、未観測の説明が他にも存在するかも知れないという事である。例えば、平均余命と一日あたりの映画鑑賞時間との間に強い相関があったとしても、「若者のほうが映画をよく観るが、死亡する可能性は低い」という説明を排除できない限り、医者が「映画をもっと観るべきだ」と勧めることはない。

### 6.1.4 重回帰

　現実のほとんどの分析では、独立変数は１つだけではない。したがって、ほとんどの数値予測タスクでは、重線形回帰（重回帰）を使うことになる。次の表に重回帰の長所と短所をまとめた。

+----------------------------------------------------------------+------------------------------------------------------------------------------+
| 長所                                                           | 短所                                                                         |
+================================================================+==============================================================================+
| -   数値データのモデル化に最もよく使われている                 | -   データについて強い仮定をおく                                             |
| -   ほぼすべてのモデル化タスクに適応できる                     | -   モデルの形状をユーザーが事前に指定しなければならない                     |
| -   独立変数（特徴量）と従属変数の関係の大きさと強さを推定する | -   欠損値に対処しない                                                       |
|                                                                | -   数値特徴量にしか対応しないため、カテゴリ値の特徴量については前処理が必要 |
|                                                                | -   モデルを理解するには統計学の知識が必要                                   |
+----------------------------------------------------------------+------------------------------------------------------------------------------+

データを読み込みます。

```{r}
launch <- read.csv('https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(3rd%20Ed.)/Chapter06/challenger.csv', stringsAsFactors = TRUE)
```

```{r}

reg <- function(y, x){
    x <- as.matrix(x)
    x <- cbind(Intercept = 1, x)
    b <- solve(t(x) %*% x) %*% t(x) %*% y
    colnames(b) <- 'estimate'
    print(b)
}
```

```{r}
str(launch)
```

```{r}
cat('単回帰分析')
reg(y = launch$distress_ct, x = launch %>% select(temperature))

cat('\n重回帰分析')
reg(y = launch$distress_ct, x = launch %>% select(-distress_ct))
```

　この重回帰分析の結果から、温度が1${}^\circ$F 上昇するごとに0.05ほど少なくなると推定している。

　新た追加された2つの独立変数もOリングのは破損数の推定に寄与している。field_check_pressure変数は打ち上げ前の検査でOリングに加えられた圧力の強さを示している。検査圧はもともと50psiだったが、一部の打ち上げでは100psiと200psiに引き上げられており、Oリングのエロージョンの引き金になったのではという見方もあった。この係数は（かなり小さいもの）正の値であり、この仮説を裏付ける小さな証拠となる。flight_numはスペースシャトルの運用期間を考慮にいれるための変数である。スペースシャトルはフライトのたびに劣化し、部品がもろくなったり故障しやすくなったりすることがある。flight_numも正の相関を持つのはこの事実を反映している可能性がある。

## 6.2 例：線形回帰を使って医療費を予測する

　この分析の目標は、患者のデータを使ってそうしたセグメントの絵費金医療費を推定することである。こちらの推定値は、予想される医療費に基づいて年なkん保険料を調整するための保険数理表の作成に利用できるはずだ。

### ステップ1：データを収集する

```{r}
# データの読み込み
insurance <- read.csv('https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(3rd%20Ed.)/Chapter06/insurance.csv', stringsAsFactors = TRUE)

insurance %>% head(n=3)
```

insurance.csvファイルには、現在医療保険に加入している患者のデータが1,338件含まれている。これらのデータは間者hの属性と保険で支払われる年間医療費を表す特徴量で構成されている。

-   age：加入者本人の年齢を示す整数（一般に医療費が無料となる65歳以上を除く）
-   sex：加入者の性別（male または female）
-   bmi：ボディマス指数（BMI）。身長と体重から算出さた肥満度を表すもので、体重（Kg ）を身長（ m）の二乗で割った値である。BMI海苔装置は18.5～24.9
-   children：医療保険の対象となる子／被扶養者の人数を表す整数
-   smoker：加入者が日常的に喫煙するかどうかを示すカテゴリ値の特徴量（yesまたはno）
-   Region：加入者の居住地（northeast、southeast、southwest、northwestの4つに分割）

### ステップ2：データの調査と前処理

```{r}
str(insurance)
```

```{r}
summary(insurance$expenses)
```

　平均値が中央値よりも大きいため、医療費の分布が右歪曲であることがわかる。

　更にヒストグラムを確認する。

```{r}
hist(insurance$expenses)
```

　カテゴリ変数になっている region を確認する。

```{r}
table(insurance$region)
```

#### 特徴量の間の関係を調べる：相関行列

　insuranceデータフレームの4つの数値特徴量から相関行列を作成するには、cor関数を使う。

```{r}
insurance %>% select(age, bmi, children, expenses) %>% cor()
```

#### 特徴量の間の関係を可視化する：散布図行列

　散布図行列は、単に散布図をグリッド上に並べたもので、3つ以上の特徴量の間でパターンを検出するために使われる。一度に可視化できる特徴量は２つだけなので、真に多次元化の可視化であるとは言えない。それでも、データの相互関係がだいたいどのようなものであるかが分かる。

　R環境にデフォルトで実装されているpairs関数で可視化してみる。

```{r}
insurance %>% select(age, bmi, children, expenses) %>% pairs()
```

　もう少し詳しく見るために psych パッケージのpairs.panels関数を使う。

```{r}
if(! require(psych)){
    install.packages('psych', quiet = TRUE)
}
library(psych)
```

```{r}
insurance %>% select(age, bmi, children, expenses) %>% pairs.panels()
```

　各散布図に描かれている楕円形の要素は**相関楕円**（correlation ellipse）であり、相関の強さを可視化している。楕円が引き伸ばされているほど、相関が強いことを意味する。bmiとchildrenの散布図にあるような正円に近い楕円は、相関は非常に弱いことを意味する（この場合は0.01）。

　ageとexpensesの散布図の楕円は、より強い相関を反映してかなり引き伸ばされている（値は0.30）。楕円の中心にある丸印は、x軸とy軸の変数の平均値を反映している。

　散布図に描かれている曲線は**Loess曲線**（loess curve）と呼ばれ、x軸とy軸の変数の一般的な関係を表す。たとえば、ageとchildrenのLoess曲線はUを逆さにした形をしており、ピークは中年層にある。つまり、高齢層や若年層のほうが中年層よりも保険プランに含まれている子供の数が少ないことを示している。この傾向は線形ではないため、相関だけでは、この関係を推測できなかった可能性がある。これに対し、ageとbmiのLoess曲線は少しずつ上に向かっており、年齢とともに体重が増えることを示唆している。

### ステップ3：モデルを 訓練する

　6つの独立変数を年間医療費に関連付ける線形回帰モデルは、次のコマンドを使って構築できる。Rのモデル式では、チルダ（\~）を使ってモデルを表す。従属変数expensesはチルダの右辺に置く。独立変数はチルダの右辺に起き、不k数の場合は+演算子で区切る。回帰モデルの切片項はデフォルトで含まれるため、指定する必要はない。

\`

```{r}
ins_model <- lm(expenses ~ age + sex + bmi + children + smoker + region, data = insurance)
```

　すべての特徴量（モデル式にすでに含まれているものを除く）を指定する場合はピリオド（.）が使えるため、このコマンドを次のように書き換える事もできる。

```{r}
ins_model <- lm(expenses ~ ., data = insurance)
```

　モデルを構築した後は、モデルオブジェクトの名前を入力するだけで、β係数の推定値を確認できる。

```{r}
print(ins_model)
```

　カテゴリ変数の場合、最初のカテゴリが基準グループとなる。このモデルでは、sexfemale、smokerno、regionnortheastの3つの変数が自動的に確保され、北東地域に住む女性の非喫煙者が基準グループとなる。したがって、残りの項目を平均すると、男性は女性に比べて年間医療費が131.40ドル少なく、喫煙者は非喫煙者に比べて年間医療費が23,847.50ドル多い。このモデルに含まれている他の３つの地域の係数は負の値であり、平均すると、４つの地域の中で基準グループ（regionnortheast）の年間医療費が最も高い傾向にあることを示唆している。

> 【Tips】
>
> デフォルトでは、Rは因子変数の最初のレベルを基準レベルにする。別のレベルを基準レベルにしたい場合は、relevel関数を使って基準レベルを明示的に指定できる。

### ステップ4：モデルの性能を評価する

　 ins_modelと入力したときに出力されたパラメータの推定値から、独立変数が従属変数にどのような影響を与えているのかはわかるが、このモデルがデータにどれくらい適合しているかは全くわからない。モデルの性能を評価するために、このモデルでsummaryコマンドを実行する。

```{R}
summary(ins_model)
```

　summaryコマンドは次の情報を出力する。

1.  **残差**\
    Residualsセクションは予測値の誤差に関する要約統計量を提供する。この出力には、見るからに大きい誤差が含まれている。残差は正解値から予測値を引いた値に等しいため、誤差の最大値が29981.7であることは、少なくとも1つの観測値について、モデルが医療費を30,000ドル近くも低く見積もっていることを示唆している。一方で、誤差の50%は1Q（第1四分位数）と3Q（第3四分位数）の間に含まれており、予測値の大部分が正解地よりも2,850.90ドル高い金額から正解値よりも、1383.90ドル低い金額の間に含まれていたことがわかる。

2.  **p値**\
    回帰係数の推定値ごとに**p値**（p-value）が出力されている。p値は$Pr(>|r|)$で示されている値であり、推定値に照らして係数の正解値が0になる確率を表す。p値が小さければ、係数の正解値が0になる可能性は非常に低い。つまり、その特徴量が従属変数と無関係である。p値の一部についている\*\*\*は、その推定値によって満たされる**有意水準**（significance level）を示す脚注に対応している。有意水準はモデルの構築前に選択された閾値であり、単なる偶然によるものではない「現実の」発見であることを示すために使われる。有意水準に満たないp値は「統計学的に有意」であると見なされる。統計学的に有意な項が少ないモデルは懸念の全員になることがある。そのモデルに使われている特徴量の説明変数としての重要度が非常に低いということになるからだ。このモデルには優位性の高い特徴量がいくつか含まれており、論理的な方法で　結果に影響を与えているように見える。

3.  **寄与率**\
    **寄与率**（multiple R-squared）は、モデル全体として従属変数の値をどれくらいうまく説明しているかを表す指数であり、**決定係数** （coefficent of determination）とも呼ばれる。この値が1.0に近づくほどモデルがデータを完全に説明するようになるという点では、相関係数に似ている。寄与率の値は0.7509なので、このモデルが従属変数の部；んさんの75%近くを説明していることがわかる。特徴量の数が多いモデルでは常に説明できる分散も増え、寄与率が1に近づくことになる。そこで、独立変数の数が多いモデルにペナルティを与えて寄与率を補正したのが**調整済み寄与率**（adjusted R-squared）である。このような補正は独立変数（説明変数）の数が異なるモデルの性能を比較するのに役立つ。

　上記の3つの性能指標を見る限り、このモデルの性能はかなりよい。現実のデータに基づく 回帰モデルでは、寄与率がかなり小さな値になることも珍しくないため、実際のところ0.75は かなり良い数字である。一部の誤差の大きさが少し気がかりだが、医療費データの性質を考え れば意外なことではない。しかし、モデルを少し異なる方法で構築すれば、モデルの性能を 改善できるかもしれない。

### ステップ5：モデルの性能を向上させる

　他の機械学習アプローチに対する回帰モデルの主な違いは、特徴量の選択とモデルの仕様を一般にユーザが決めるという点にある。したがって、特徴量と従属変数の関係についての知識があれば、その情報をモデルの仕様に反映させモデルの性能を改善できることがある。

#### モデルに非線形関係を追加する

　線形回帰では、独立変数と従属変数の関係が線形であると仮定するが、この仮定が常に成り立つとは限らない。たとえば、年齢が医療費に与える影響は、すべての年齢値に渡って一定であるとは限らない。高齢者の医療費は不釣り合いなほど高くなることがある。 　一般的な回帰式が次の形式を取る。 $$
y = \alpha +  \beta_1x
$$ 　非線形関係を考慮するには、回帰式に高次の項を追加することで、モデルを多項式として扱えばよい。 実質的には、次のような関係をモデル化することになる。 $$
y = \alpha + \beta_1x + \beta_2x^2
$$ 　これらの2つのモデルの違いは、$\beta$をもう1つ推定することで$x^2$の項の影響を補足することにある。このようにすると、ageの影響をあageの二条の関数として表せるようになる。 　 ageの非線形性をモデルに追加するには、新しい変数age2を作成すればよい。

```{r}
insurance <- insurance %>% mutate(age2 = age^2)
insurance %>% select(age, age2) %>% head(n = 5)
```

　続いて、新しいモデルを構築するときに、expenses \~ age + age2 というモデル式を使ってage と age2 を lm関数に追加する。

#### 数値特徴量を二値の変数に変換する

　特徴量影響が影響力を持つのは特定の閾値に達したときだけで、その影響力は累積的ではないと にらんでいるとしよう。たとえば、bmiが医療費に与える影響が正常な体重の範囲では ゼロだったとしても、肥満（値が30以上）の場合は医療費の上昇に強い影響を与えているかもしれない。 　この関係をモデル化するために、二値の肥満指標変数を作成する。この変数の値は、bmiの値が30以上の場合は1、30未満の場合は0である。この二値変数の $\beta$係数の推定値は、（bmiの値が30未満の人を基準として） bmiの値が30以上の場合に医療費に与える平均的な影響の大きさを示すものになる。

　この変数はifelse関数を使って作成できる。この関数は、ベクトルの要素ごとに指定された条件をテストし、その条件が満たされたかどうかに基づいて値を返す。bmiの値が30以上の場合は1を返し、それ以外の場合は0を返す変数は次のようになる。

```{r}
insurance <- insurance %>% mutate(bmi30 = ifelse(.$bmi >= 30, 1, 0))
```

　あとは、このbmi30変数を新しいモデルに追加すればよい。BMIの線形の影響とは別に肥満の影響があると考えるかどうかに応じて、この変数を単に追加するか、元のbmi変数の代わりに使うことができる。

> 【Tips】
>
> 変数をツキカするかどうかで悩んでいる場合の常套手段は、変数追加してp値を調べることである。ヘンスが統計学的に有利でなければ、あとからこの変数を除外する根拠となる。

#### モデルに相互作用を追加する

　ここまでは、従属変数への寄与をもっぱら特徴量ごとに検討してきた。しかし、特徴量の特定の組み合わせが従属変数に影響を与えることもある。たとえば、喫煙と肥満はどちらも悪影響を及ぼす可能性があるが、喫煙と肥満の複合的な影響のほうが、それぞれの影響を足し合わせたものよりも大きいと捉えられる場合がある。 　２つの特徴量が複合的な影響力を持つことを相互作用（interaction）と呼ぶ。２つの特徴量に相互作用があると推測される場合は、それらの相互作用をモデルに追加して、実際に相互作用があるかどうかを確かめることができる。これをRのモデル式で指定するには、肥満指標（bmi30）と喫煙指標（smoker）の相互作用を追加するには、`expeses ~ bmi30 * smoker`というモデル式を記述する。 　\*演算子は、`expenses ~ bmi300 + smokeryes + bmi30:smokeryes`をモデル化するための省略表記である。この展開式に含まれている:演算子は、bmi30:smokeryesが2つの変数の相互作用であることを示している。また、この展開式には、bmi30とsmokeryesの相互作用に加えて、これらの変数も自動的に含まれている。

> 【Tips】
>
> 相互作用を持つ変数もそれぞれにモデルに追加する必要がある。そうでなければ、相互作用モデルに追加すべきではない。常に\*演算子を使って相互作用を追加するなら問題はない。その場合は、必要な変数が自動的に追加されるからだ。

#### すべてを組み合わせて回帰モデルを改善する

　医療費と患者の属性との関係に関する知識を活かして、回帰式を改善した方法をまとめる。

-   年齢に関する非線形項の追加
-   肥満を表す二値変数の作成
-   肥満と喫煙の相互作用の追加

　先程と同様にlm関数を使ってモデルを訓練するが、今回は新たに作成した変数と相互作用の項を追加する。

```{r}
ins_model2 <- lm(expenses ~ age + age2 + children + bmi + sex +
                  bmi30*smoker + region, data = insurance)

summary(ins_model2)
```

　最初のモデルと比較すると、寄与率が0.75から0.87に改善されている。 　同様に、モデルが少し複雑になったことを考慮するための調整済み寄与率も0.75から0.87に 改善されている。このモデルは医療費の分散の87%を説明できている。それに加えて、モデルの関数形 についての論理の論理も検証されたようである。高次の項age2は統計学的に有意であり、肥満指標 bmi30も統計学的に有意である。肥満と喫煙の相互作用bmi30:smokeryesは多いな影響力を持つようである。 喫煙だけでも医療費が13,404ドルに増えるが、肥満と喫煙では医療費が19,810ドルも増える。このことは、 肥満が原因の疾患が喫煙によって悪化することを示唆しているかもしれない。

> 【Tips】
>
> 回帰モデルの価値は、プロセスを正確に捉えるかどうかではなく、予測が正確かどうかで決まる。しかし、回帰モデルの係数から確実な予測を行いたい場合は、回帰の仮定が覆されていないことをテストで診断する必要がある。

#### 回帰モデルを使って予測を行う

　回帰係数の推定値とモデルの統計量を調べた後は、このモデルを使って医療保険プランに将来加入する人の医療費を予測することもできる。予測をどのように行うのかをぐち的に示すために、まず、predict関数を使ってモデルを最初の訓練データに適用してみよう。

```{r}
insurance <- insurance %>% mutate(pred = predict(ins_model2, .))
```

　このコマンドを実行すると、予測値がpredという新しいベクトルとしてinsuranceデータフレームに保存される。 続いて、医療費の予測値と正解値の相関係数を計算する。

```{r}
cor(insurance$pred, insurance$expenses)
```

　相関係数の0.93という値は、予測値と正解値の間に非常に強い線形関係があることを示唆している。 これはよい兆候であるーモデルの正解率が高いと考えられるからだ。また、この結果を散布図で調べて見るのも 効果的である。この関係をプロットし、切片0、傾き1の対角線を引く。col、lwd、ltyつのパラメータはそれぞれ線の色、幅、種類をしている

```{r fig.height=5, fig.width=6}
plot(insurance$pred, insurance$expenses)
abline(a = 0, b = 1, col = 'red', lwd = 3, lty = 2)
```

　対角線よりも上にある点は、実際の医療費が予測値を上回ったケースを表している。立っ各線よりも下にある点は、医療費が予測値を下回ったケースである。医療費かが予測を大きく上回っている加入者の数は、医療費が予測を少し下回っている大多数の加入者に見合うほど少ない事がわかる。 　ここで、医療保険プランの新しい加入者の医療費を予測したいとしよう。異旅費を予測するには、加入者候補のデータを含んだデータフレームをpredict関数に渡さなければならない。加入者の数が多い場合は、CSVファイルを作成してRに読み込むことを検討したほうがよいだろう。加入者の数が少ない場合は、predict関数の呼び出し時にデータフレームを作成すればよい。たとえば、30歳、肥満、非喫煙者、子供2人、北東地域在住の男性の医療費を予測するコマンドは次のようになる。

```{r}
predict(ins_model2,
        data.frame(age = 30, age2 = 30^2, children = 2, bmi =30,
                   sex = 'male', bmi30 = 1, smoker = 'no', region = 'northeast'))
```

　医療保険会社は、この値をもとに、この加入者グループの損益分岐点を年間6,000ドル（月額500ドル）に設定する必要があるかもしれない。predict関数を同じように使って、女性であること以外は同じデータを持つ加入者の医療費と比較してみよう。

```{r}
predict(ins_model2,
        data.frame(age = 30, age2 = 30^2, children = 2, bmi =30,
                   sex = 'female', bmi30 = 1, smoker = 'no', region = 'northeast'))
```

　これらの２つの予測値の差は$5,973.774 - 6,470.543=496.769$であり回帰モデルの係数の推定値と同じであることがわかる。性別 以外の条件がすべて同じであるとすれば、 男性の年間医療費は平均して496ドルほど少ない計算になる。 　このことから具体的にわかるのは、「医療費の予測値は各回帰係数にデータフレームの 対応する値を掛けた値の合計である。」というより一般的な事実である。たとえば、 childrenに対する回帰係数678.6017を使って、子供の人数を2から0に減らすと、医療費が $2*678.6017=1,357.203$ドル少なくなることを予測できる。

```{r}
predict(ins_model2,
        data.frame(age = 30, age2 = 30^2, children = 0, bmi =30,
                   sex = 'female', bmi30 = 1, smoker = 'no', region = 'northeast'))
```

```{r}

cat(6470.543 - 5113.34)
```

> 【Tips】
>
> モデルの回帰係数をエクスポートすれば、予測関数を独自に構築できるようになる。 そうした予測関数の用途としては、顧客データベースでリアルタイムに予測を行う 回帰モデルの実装が考えられる。

## 6.3 回帰木とモデル木を理解する

　決定木はフローチャートとよく似たモデルを構築する。決定木を構築するアルゴリズムを ほんの少し調整すれば、このような決定木を数値の予測にも利用できる。数値を予測する決定木と 分類用の決定木を比較する。 　数値を予測する決定木は２つのカテゴリーに分類される。１つ目は**回帰木**（regression tree）と 呼ばれるもので、**CART**（Classification and Regression Tree）という画期的なアルゴリズムの一部として 1980年代に発表された。その名前に反して、回帰木は本章で説明してきた線形回帰の手法を使わない。代わりに、 葉ノードに到達したインスタンスの平均値に基づいて予測値を生成する。

　数値を予測するための２つ目の決定木は、**モデル木**（model tree）と呼ばれる。 回帰木の数年後に発表され、知名度も低いが、おそらくこちらのほうが強力である。モデル木は 回帰木とほぼ同じように成長するが、葉ノードごとに、そのノードに到達したインスタンスから重回帰モデルを 構築する。葉ノードの数によっては、そのような重回帰モデルが数十あるいは数百も作られることがある。このため、 同じデータで構築され高い木々よりも理解するのが難しいが、より性能の良いモデルが得られる可能性がある。

> 【Note】 最も初期のモデル木アルゴリズムはM5である

### 6.3.1 決定木に回帰を追加する

　数値を予測できる決定木は、回帰モデルの代わりに利用できる選択肢となるが、見落とされがちである。 次の表に回帰木とモデル木をより一般的な線形回帰法と比較したときの長所と短所をまとめた。

+----------------------------------------------------------------------------+--------------------------------------------------------------------+
| 長所                                                                       | 短所                                                               |
+============================================================================+====================================================================+
| -   決定木の長所と数値データをモデル化する能力を併せ持つ                   | -   線形回帰ほどよく知られていない                                 |
| -   ユーザーがモデルを事前に指定する必要がない                             | -   大量の訓練データが必要                                         |
| -   特徴量選択を自動的に行うため、特徴量の数が非常に多い場合でも利用できる | -   個々の特徴量が従属変数に与える全体的な影響がわかりにくい       |
| -   データの種類によっては、線形回帰よりもはるかにうまく適合することがある | -   決定木が大きくなると回帰モデルよりも解釈が難しくなることがある |
| -   統計学の知識がなくてもモデルを解釈できる                               |                                                                    |
+----------------------------------------------------------------------------+--------------------------------------------------------------------+

　数値予測タスクでは、一般に従来の回帰法が最初の選択肢となる。しかし、場合によっては数値を予測する決定木のほうが明らかに有利である。たとえば、特徴量の数が多いタスクや、得料料と従属変数の間に複雑な非線形関係がいくつも 認められるようなタスクには、決定木のほうが適していることがある。このような状況で回帰を用いるのは困難の もとである。また、回帰モデルはデータについて仮定をおくが、それらの仮定は現実のデータに即していない事が多い。 決定木には、そのような問題はない。

　分類木では均一性をエントロピーで計測したが、数値データではエントロピーを計算できない。そのため、 分散、標準偏差、または平均からの絶対偏差といった統計量を使う。

　よく使う分割基準の１つに、次の式で定義される**SDR**（Standard Deviation Reduction）がある。

$$SDR=sd(T)-\sum{i\frac{|T_i|}{|T|}}\times sd(T_i)$$

　ここで、$sd(T)$関数は集合$T$に含まれる値の標準偏差を表し、$T_1, T_2 \dots T_n$は特徴量に基づく 分割の結果として得られた値の集合を表す。$|T|$項は集合$T$の観測値の個数を表す。基本的には、分割前の 標準偏差と分割後の重み付けされた標準偏差を比較することで、標準偏差がどれくらい減少するのかを計測する。

```{r}
# 元データ
tee <- c(1, 1, 1, 2, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7)

# 特徴量Aで分割
at1 <- c(1, 1, 1, 2, 2, 3, 4, 5, 5)
at2 <- c(6, 6, 7, 7, 7, 7)

# 特徴量Bで分割
bt1 <- c(1, 1, 1, 2, 2, 3, 4)
bt2 <- c(5, 5, 6, 6, 7, 7, 7, 7)

# それぞれの分割後の標準偏差
sdr_a <- sd(tee) - (length(at1) / length(tee) * sd(at1) +
                      length(at2) / length(tee) * sd(at2))
sdr_b <- sd(tee) - (length(bt1) / length(tee) * sd(bt1) +
                      length(bt2) / length(tee) * sd(bt2))

cat('sdr_a   ', sdr_a, '\n')
cat('sdr_b   ', sdr_b)


```

　特徴量Aで分割したときのSDRが約1.2であるのに対し、特徴量Bで分割したときのSDRは約1.4である。特徴量Bで文なkつするほうが標準偏差が小さくなるため、決定木はまずBを使う。このようにすると、特徴量Aで分割するよりも少し均一性が良くなる。

　この一回限りの分割を行ったところで、決定木の成長が止まったとしよう。回帰木の作業はここで終了となり、特徴量Bの値ををグループ $T_1$か$T_2$のどちらかに分類することで、新しいインスタンスの値を予測できる。 インスタンスが$T_1$に分類された場合、回帰木の 予測値は $mean(bt1)=2$となり、$T_2$に分類された場合には$mean(bt2)=6.25$となる。

　これに対し、モデル器はもう一歩先へ進む。モデル木では、グループ$T_1$に分類された７つの訓練データをグループ$T_2$に分類された ８つの訓練データを使って、従属変数と特徴量Aの県警回帰モデルを構築できる。なお、特徴量Bは回帰モデルの構築には役立たない。 なぜなら、インスタンスは特徴量Bの値はすべて同じだからだ。モデル器は2つの線形モデルとのどちらかを使って新しいインスタンスの値を 予測できる。

## 6.4 例：回帰木とモデル木を使ってワインの品質を予測する

　ワインの醸造は困難にして競争の激しい事業だが、大きな利益を生む可能性がある。とはいえ、ワイナリーの収益に貢献する要因は さまざまである。ワインは農産物で造られるため、天候や育成環境といったさまざまな変数がワインの品質に影響を及ぼす。 瓶詰めや製造工程もワインの味わいに良くも悪くも影響を与えることがある。ボトルのデザインや価格k設定といったマーケティングの 方法までもが顧客による味の認識を左右することがある。

　このため、ワイン業界はワイン製造の決定科学の助けになるかもしれないデータ収集と機械学習の手法に多額の投資を行っている。 たとえば、様々な地域で製造されたワインの化学成分の主な違いを発見したり、ワインの糖度が上がる化学的要因を特定するために 危害学習が使われている。

　ワインの品質を評価するのは難しいことで知られているが、最近では、この作業を補助する ために機械学習が使われいる。ワインが一級品となるかどうかが有名なワイン評論家のレビュー によって決まるというのはよくあることだが、当の専門ですら、ブラインドテストでの 評価には一貫性がない。

　このケーススタディでは、回帰木とモデル木を使って専門家によるワインの評価を模倣でき るシステムを作成する。決定木をツアkったモデルは理解しやすいため、ワインの評価を上げるの に貢献する主な要因をワインメーカーが特定できるはずだ。それよりも重要なのは、このシステムが 評価者の気分や味覚の麻痺といった人的な用悪阻に左右されないことだろう。したがって、 コンピュータベースのワインテストでは、より客観的で、一貫した公平な評価が得られるだけではなく、 ワインの品質もよくなる可能性がある。

### ステップ1：データを収集する

　白ワインのデータを読み込む。このデータは実験的分析により、酸度、糖度、塩化物、硫黄分、 アルコール度、pH、密度といった成分がワインごとに計測されている。これらのサンプルは3人以上の パネラーによるブラインドテストにより、0（非常に悪い）から10（すばらしい）までのレートで 評価されている。

```{r}
wine <- read.csv('https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(3rd%20Ed.)/Chapter06/whitewines.csv', stringsAsFactors = TRUE)

wine %>% head(n = 3)
```

### ステップ2：データの調査と前処理

　これまでと同様に、read.csv関数を使ってデータをインポートする。特徴量はすべて数値であるため、stringAsFactorsパラメータは無視しても安全である。

```{r}
str(wine)
```

　モデルの性能を評価するには、従属変数の分布を調べる必要がある。たとえば、ワインの品質にそれほど大きな違いがない、あるいは非常によいか悪いかの二峰分布に従っているとしよう。このことはモデルの設計方法に影響を与えるかもしれない。そのような極端な分布をチェックしたい場合は、ヒストグラムを使ってワインの品質の分布を調べることができる。

```{r}
hist(wine$quality)
```

　ワインの品質は６の値を中心とする釣鐘型の正規分布に従っている。このデータセットはランダムに並んでいるために、学習データ、評価データの分割はそのまま２つのデータに分割する。

```{r}
wine_train <- wine[1:3750, ]
wine_test <- wine[3751:4898, ]
```

### ステップ3：モデルを訓練する

　回帰木モデルを訓練する。回帰木モデルの構築には決定木のほぼすべての実装を利用できるが、CARTチームが定義した回帰木を最も忠実に実装しているのはrpartパッケージである。

```{r}
# ライブラリの読み込み
if(! require(rpart)){
    install.packages('rpart', quiet = TRUE)
}
library(rpart)
```

　今回作成する回帰木モデルには、後ほど訓練するモデル木と区別するためにm.rpartという名前をつける。

```{r}
m.rpart <- rpart(quality ~ ., data = wine_train)

m.rpart
```

　回帰木のノードごとに、決定ノードに到達したインスタンスの個数が出力されている。たとえば、ルートノードから出発した3,750個のインスタンスのうち、2,372個がalchol \< 10.85に分類されており、1,378個がalcohol \>= 10.85に分類されている。　alcoholは、この回帰木で最初に使われた独立変数なので、ワインの品質にとって最も重要な説明変数はアルコール度数ということになる。

　\*がついているノードは葉（終端）ノードであり、そこで予測値（yval）が生成されることを意味する。たとえば、ノード5のyvalは5.971091である。したがって、この回帰木を使って予測を行うとしたら、alcohol \< 10.85かつvolatile.acidity \< 0.2275のインスタンスのqualityは5.97と予測されることになる。

　なお、各ノードの平均二乗誤差や特徴量の全体的な重要度など、回帰木の適合度に関するより詳細な要約統計量は、summary（m.rpart）コマンドを使って取得できる。

#### 回帰木を可視化する

　この回帰木は先ほどの出力だけでも十分に理解できるが、可視化を使ったほうが理解しやすいことが多い。rpart.plotパッケージは、高品質な決定木を出力する使いやすい関数を提供する。

```{r}
# ライブラリの読み込み
if(! require(rpart.plot)){
    install.packages('rpart.plot', quiet = TRUE)
}
library(rpart.plot)
```

　次のコマンドを使って、決定木を図示することができる。

```{r}
rpart.plot(m.rpart, digits = 3)

```

　決定木図に表示される数値の桁数を制御するdigitsパラメータの他にも、可視化の様々な部分を調整できる。

```{r}
rpart.plot(m.rpart, digits = 4, fallen.leaves = TRUE, type = 3, extra = 101)
```

　fallen.leavesパラメータは、葉ノードをプロットの最下部に一列に並べる。typeパラメータとextraパラメータは、決定ノードや葉ノードのラベル表示を制御する。type = 3とextra = 101は特定のスタイルフォーマットを表す。

### ステップ4：モデルの性能を評価する

　回帰木モデルを使ってテストデータで予測値を生成するには、predict関数を使う。デフォルトでは、この関数は従属変数の予測値を数値で返すため、 p.rpartというベクトルに保存する。

```{r}
p.rpart <- predict(m.rpart, wine_test)
```

```{r}
summary(p.rpart)
```

```{r}
summary(wine_test$quality)
```

　このモデルあh、たとえば最も評価の高いワインといった極端なケースをうまく特定できないようである。一方で、第1四分位数と 第3四分位数の間はうまく予測できているようだ。

　予測値と正解値の相関は、モデルの性能を測る単純な手段となる。corかんすうをりようすれあ、同じ長さの2つのベクトル間の関係を 数値化できることを思い出そう。この関数を使って予測値が正解値にどれくらい対応しているのかを調べてみよう。

```{r}
cor(p.rpart, wine_test$quality)
```

　0.54という相関は、確かに受け入れられない数字ではない。しかし、相関からわかるのは予測値と正解値の関係の強さだけで、 予測値が正解地からどれくらい離れているのかはわからない。

#### 平均絶対誤差を使って性能を評価する

　モデルの性能について検討するもう１つの方法は、平均すると、予測値が正解値からどれくらい離れているのかを調べること である。今指標は**平均絶対誤差**（mean absolute error: MAE）と呼ばれる。MAEの式は次のようになる。ここで、$ｎ$は予測値 の個数を表し、$e_i$は予測値$i$の誤差を表す。

$$
MAE=\frac{1}{n}\sum_{i=1}^{n}{|e_i|}
$$

　MAEという名前からもわかるように、この式は誤差の絶対値の平均を求めている。誤差は単に予測値と平均値の差であるため、 単純なMAE関数を次のように作成できる。

```{r}
MAE <- function(actual, predicted){
  mean(abs(actual - predicted))
}
```

　これで、予測値のMAEを次のコマンドで取得できる。

```{r}
MAE(p.rpart, wine_test$quality)
```

　つまり、モデルの予測値と正解値（実際のスコア）の差は、平均すると約0.59である。 品質の尺度は0から10であるため、このモデルの性能はかなり良いようだ。

　一方で、ほとんどのワインが特によいわけでも悪いわけでもないことを思い出そう。一般的な品質スコアは 5～6である。したがって、この指標に従うと、平均値しか予測しない分類器であってもかなり性能がよいことに なってしまう。

　訓練データセットでの品質スカの平均値を見てみよう。

```{r}
mean(wine_train$quality)
```

　全てのワインサンプルで5.87を予測したとすれば、MAEは約0.67になるはずだ。

```{r}
MAE(5.87, wine_test$quality)
```

　平均すると、この回帰木（MAE = 0.59）は平均値しか予測しない分類器（MAE = 0.67） よりは正解値に近いが、たいした差ではない。Cortezのちょうさでは、ニューラルネットワークモデルのMAEは0.58、 サポートベクトルマシンのMAEは0.45である。このことは、まだ改善の余地があることを示している。

### ステップ5：モデルの性能を向上させる

　回帰木の性能を向上させるために、モデル木を適用してみよう。モデル木は数値を予測するために決定木をより複雑に応用 したものである。モデル木が葉ノードを回帰モデルに置き換えることで回帰木を拡張することを思い出そう。 このため、葉ノードで数値の独立変数を1つしか使わない回帰木よりも高い性能が得られることが多い。 　現時点において最先端のモデル木はCubistアルゴリズムである。このアルゴリズムはモデル木アルゴリズムM5の拡張であり 、どちらも1990年代前半に発表された。Cubistアルゴリズムは決定木を構築し、決定木の枝に基づいて決定ルールを作成し、 決定木の枝に基づいて決定ルールを作成し、それぞれの葉ノードで回帰モデルを構築する。予測値の質や予測値の範囲の 滑らかさを向上させるために、剪定やブースティングといったヒューリスティクスを用いている。

```{r}
if(! require(Cubist)){
  install.packages('Cubist', quiet = TRUE)
}

library(Cubist)
```

```{r}
m.cubist <- cubist(x = wine_train[-12], y = wine_train$quality)

m.cubist
```

　この出力から、このモデルがワインの品質をモデル化するために25のルールを作成したこ とがわかる。これらのルールの一部を調べるために、モデルオブジェクトにsummary関数を 適用してみよう。完全なモデル木は非常に大きいため、ここでは最初の決定ルールを表してい る部分だけを調べる。

```{r}
summary(m.cubist)
```

　出力のif部分が先程構築した回帰木に少し似ていることがわかる。ワインの４つの成分 （free.sulfur.dioxide、 total.sulfur.dioxide、sulphates、alcohol）を使った一連の 決定に基づいて最終的な予測を行うためのルールを作成している。このモデル器の出力と回帰 木の出力の主なちがいは、これらのノードが数値予測ではなく線形モデルを表していることだ。

　このルールの線形モデルは then に続く outcome = 分に含まれている。これらの数字の解釈 は本章で重回帰モデルを構築したときと全く同じである。各数字は対応する特徴量の$\beta$係数 の推定値である。つまり、その特徴量が従属変数（ワインの品質）の推定値に与える影響の 大きさを表している。たとえば、 residual.sugar の0.186だけ高くなることを意味する。

　ここで注意しなければならないのは、このモデルが推定する回帰効果が適用される対象が、 このノードに到達したワインサンプルに限られることである。Cubistモデルの出力全体を調べて みると、このモデル木では、決定ルールごとに１つ、合計で25個の線形モデルが構築され ていることがわかる。そして、 residual.sugar をはじめとする特徴量の影響を表す推定値は モデルごとに異なっている。

　このモデルの性能を評価するために、未知のテストデータでの性能を調べてみよう。 predict 関数は予測値からなるベクトルを返す。

```{r}
p.cubist <- predict(m.cubist, wine_test)
```

　モデル木は回帰木よりも広い範囲の値を予測しているようだ。

```{r}
summary(p.cubist)
```

　相関もかなり強くなっているようである。

```{r}
cor(p.cubist, wine_test$quality)
```

　さらに、MAE も少し減少している。

```{r}
MAE(wine_test$quality, p.cubist)
```

　回帰木の性能を大きく改善することはできなかったが、 Cortezが発表したニューラルネットワークの 性能は上回っており、サポートベクトルマシンの0.45という MAE 値に少し近づいている。 しかも、ニューラルネットワークやサポートベクトルマシンよりもはるかに単純な学習法を使っている。
