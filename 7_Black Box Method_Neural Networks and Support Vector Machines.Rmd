---
title: "7_Black Box Method_Neural Networks and Support Vector Machines"
output: inline
---

# 第7章 ブラックボック手法ーニューラルネットワークとサポートベクトルマシン

　本章では次の内容を取り上げる。

-   数学関数をモデル化するために脳の動きを模倣するニューラルネットワーク

-   多次元表面を使って特徴量と目的変数の関係を定義するサポートベクトルマシン

-   複雑ではあるものの、これらの手法を現実の問題に簡単に応用できること

## 7.1 ニューラルネットワークを理解する

　ANNは汎用的な学習器であり、分類、数値予測、さらには教師なし学習のパターン認識まで、 ほぼあらゆるタスクに応用できる。\
　ANNは入力データと出力データが明確に定義されていることが多いが、入力を 出力に関連付けるプロセスが非常に複雑で、簡単に定義できないような問題にも適用される。 ブラックボックス手法であるANNは、このようなブラックボックス問題に適している。

### 7.1.1 生物学上のニューロンから人工ニューロンへ

　人工ニューロンが１つだけのモデルは、生物学上のモデルに非常によく似たものとして 理解できる。\
　n個の入力樹状突起を持つ一般的な人工ニューロンは、次の式で表すことができる。 n個の入力（$x_i$で表される）は、重み$w$に従い、それぞれ入力信号の総和に多かれ 少なかれ影響を与える。この総和を活性化関数$f(x)$を与えられたときに得られる信号 $y(x)$が出力軸索となる。

$$
y(x)=f\bigg(\sum_{i-1}^{n}{w_ix_i}\bigg)
$$

　ニューラルネットワークは、このようにして定義されたニューロンを構成要素として、複雑なデータモデルを構築する。ニューラルネットワークにはさまざまな種類があるが、それぞれ次の特徴を持つものとして定義できる。

-   **活性化関数**\
    ニューロンの総入力信号をネットワークにブロードキャストされる単一の出力信号に変換する。
-   **ネットワークトポロジ**（ネットワークアーキテクチャ）\
    モデルに含まれるニューロンの個数と、層の個数をそれらの層がどのように結合されるのかを表す。
-   **訓練アルゴリズム**\
    入力信号に比例する形でニューロンを抑制する、またはニューロンに刺激を与えるために、結合の重みをどのように設置するのかを指定する。

### 7.1.2 活性化関数

　活性化関数は、人工ニューロンが入力情報を処理してネットワークに伝達するためにのメカニズムである。\
　生物学的には、「総入力信号を蓄積し、発火の閾値に達したかどうかを判断するプロセス」として活性化関数をイメージするとよいかもしれない。閾値に達した場合、ニューロンは信号を伝達し、閾値に達していない場合は何もしない。ANNの用語では、指定された入力閾値に達した場合のみ出力信号を伝達することから、このような活性化関数を**閾値活性化関数**（threshold activation function）と呼ぶ。閾値活性化関数は**単位ステップ活性化関数**（unit step activation function）とも呼ばれる。

$$
f(x) = 
   \left\{
      \begin{align*}
         &0 \ \ \  if\ \ x<0 \\
         &1 \ \ \  if\ \ x \geq 0
      \end{align*}
   \right.
$$

　閾値活性化関数は、生物学との類比という点では興味深いが、ANNで使われることは滅多ににない。\
　おそらく、最もよく使われる活性化関数は、**シグモイド活性化関数**（sigmoid activation function）だろう。\
　シグモイド関数は**微分可能**（differentiable）であり、入力値の範囲全体に対する導関数を計算できる。

　活性化関数のデフォルトとしてはシグモイド関数を使われることが多いが、その他に線形関数・飽和型線形関数・双曲線正接関数・ガウス関数などの活性化関数を利用できることがある。\
　これらの活性化関数を区別するうえで最も重要となるのは、出力信号の範囲である。一般には、(0,1)、(-1,+1)、(-inf,+inf)のいずれかを使う。活性化関数の選択は、そのニューラルネットワークがどのような種類のデータと適合するのかを方向付ける。 これにより、データの種類に特化したニューラルネットワークの構築が可能となる。\
　たとえば、線形活性化関数を選択した場合、ニューラルネットワークは線形回帰モデルに非常によく似たものとなる。ガウス活性化関数は**動径基底関数**（radial basis funtion:RBF）ネットワークのベースとなる。それぞれに特定の学習タスクとの相性がある。\
　多くの活性化関数では、出力に影響を与える入力信号の値の範囲が比較的狭い。この点を認識しておくことは重要である。たとえば、シグモイド関数の場合は、入力信号が-5以下なら出力信号は0であり、+5以上であれば出力は1である。信号をｋのような範囲に押し込むと、入力の上限と下限の変動幅が大きい（絶対値が大きな値をとる）要な場合に出力が飽和し、一定の値になってしまう。ギターのアンプの出力を上げすぎたときに音声信号の上部がカットされ、音が歪むようなものだ。基本的には、入力値を狭い範囲の出力値に 押しつぶすことになるため、シグモイド関数のような活性化関数を**押しつぶし関数**（squashing function）と呼ぶことがある。\
　押しつぶし問題に対する解決策の1つは、ニューラルネットワークの入力値をすべて変換し、たとえば特徴量の値が0を中心とする狭い範囲に収まろうとすることである。要するに、特徴量を標準化または正規化をするようなものだ。入力値の範囲を制限することにより、 活性化関数がその範囲全体を適切に処理できるようになる。また、入力値が適切な範囲にまとまっていると、アルゴリズムの 処理がより高速になるため、訓練にかかる時間が短くなるという効果もある。

### 7.1.3 ネットワークトポロジ

　ニューラルネットワークの学習能力は、その**トポロジ**（topology）に根ざしている。トポロジとは、相互接続されたニューロン のパターンまたは構造のことである。ネットワークアーキテクチャは無数に存在するが、次の3つの主な特徴に基づいて区別できる。

-   層の個数
-   ネットワーク内の情報を逆方向に伝達できるか
-   ネットワークの各層に含まれているノードの個数

　ニューラルネットワークが学習できるタスクの複雑さはトポロジによってきまる。一般に、 ネットワークがより大規模で複雑なものになるほど、より微細なパターンや複雑な決定境界を 識別でkるようになる。しかし、ネットワークの性能はネットワークの規模だけで決まるわけ ではなく、ユニットが配置される方法にも左右される。

#### 層の個数

　トポロジを定義するには、人工ニューロンをネットワーク内での位置に基づいて区別する用語 が必要だ。**入力ノード**（input node）と呼ばれる一連のニューロンが、入力データからの未処理の信号を直接受け取っている。 各入力ノードはデータセット内の特徴量を1つだけ処理する。特徴量の値は対応するノード の活性化関数によって変換される。入力ノードが送信した信号は**出力ノード**（output node） で受診され、出力ノードの活性化関数が最終的な予測値$(p)$を生成する。

　入力のノードと出力ノードは**層**（layer）と呼ばれるグループにまとめられる。入力ノードは受信した データをそのまま処理するため、ネットワークの結合の重みは1セットしかない（この場合は、$w_1$、$w_2$、$w_3$）。このため、 **単層ネットワーク**（single-layer network）と呼ばれる。 単層ネットワークは基本的なパターン分類（特に線形分離可能なパターン）に利用できるが、 ほとんどの学習タスクでは、もっと高度なネットワークが必要になる。\
　より複雑なネットワークの作成と聞いてすぐに思い浮かぶのは、層を追加することである。 **隠れ層**（hidden layer）を1つ以上追加したネットワークを**多層ネットワーク**（mulilayer network）と呼ぶ。 隠れ層は入力ノードからの信号を処理した上で出力ノードに伝達する。 ほとんどの多層ネットワークは**全結合**（fully connected）であり、ある層のすべての ノードが次の層のすべてのノードと結合している。ただし、全結合である必要はない。

#### 情報が伝達される向き

　ここまでの例では、信号が一方向にのみ流れることを示すために矢印を使ってきた。入力信号が 入力層から出力層に向かって（ループせずに）連続的に流れるネットワークを**フィードフォワードネットワーク** （feedfoward network）と呼ぶ。\
　情報の流れが制限されているにもかかわらず、フィードフォワードネットワークは驚くほど柔軟である。 たとえば、レベルの個数や各レベルのノードの個数が違っててもよいし、 同時に複数の目的変数をモデル化することも、複数の隠れ層を適用することもできる。 複数の隠れ層を持つニューラルネットワークを**ディープニューラルネットワーク**（deep neural network : DNN）と呼び、このようなネットワークを訓練することを**ディープラーニング**または**深層学習**（deep learning）と呼ぶ。画像認識やテキスト処理といった複雑なタスクでは、大規模なデータセットで訓練した DNNの性能が人間の能力に匹敵することがある。

　フィードフォワードネットワークとは対象的に、**リカレントネットワーク**（recurrent network）では、ループを使って 信号を逆向きに伝達できる。このため、リカレントネットワークは**フィードバックネットワーク**（feedback network ) とも呼ばれる。この特性は生物学上のニューラルネットワークの仕組みに近いもので、極めて複雑なパターンの学習が可能になる。リカレントネットワークの能力は短期記憶（遅延）の追加によって飛躍的に向上する。これには時系列的な事象を理解する能力が含まれており、株価の予測、音声認識、天気予報などでの利用が考えられる。

　よく知られるアプリケーションでDNNとリカレントネットワークが使われるケースは増える一方であり、結果としてDNNとリカレントネットワークの人気は高まっている。しかし、このようなネットワークの構築には、専用の計算ハードウェアやクラウドサーバーが必要となる事が多い。一方で、多くの現実的なタスクのモデル化には、より単純なフィードフォワードネットワークでも十分対応できる。実際には、多層のフィードフォワードネットワークは**多層パーセプトロン**（mulilayer perceptron : MLP）とも呼ばれ、ANNトポロジのデファクトスタンダードとなっている。

#### 各層のノードの個数

　ニューラルネットワークの複雑さは、層の個数や情報が流れる向きだけではなく、各層のノードの個数によっても異なることがある。入力ノードの個数は、入力データの特徴量の個数によってあらかじめ決まっている。同様に、出力ノードの個数は、モデル化する目的変数の個数や目的変数のクラスレベルの個数によってあらかじめ決まっている。しかし、隠れノードの高尾数は、モデルを訓練する前にユーザが決めることができる。\
　残念ながら、隠れ層のニューロンの個数を決める確かな法則はない。ニューロンの適切な個数は、入力ノードの個数、訓練データの量、ノイズが多いデータの量、学習タスクの複雑さなど、様々な要因に左右される。\
　一般に、ネットワークトポロジーはネットワークの結合の数が多いほど複雑になり、ネットワークトポロジが複雑になるほど、より複雑な問題を学習できるようになる。ニューロンの数が多いほど、訓練データをより忠実に反映したモデルになるが、これには過学習というリスクが有る。このため、未知のデータにうまく汎化しないモデルになることがある。また、ニューラルネットワークの規模が大きくなれば、それだけ計算量も増えるため、訓練に時間がかかることがある。\
　ベストプラクティスは、訓練データセットで十分な清野が得られるギリギリの数までノードを減らすことがある。ホトのンドの場合は、隠れノードの数がほんのわずか（数えるほど）であっても、ニューラルネットワークは途方もない学習能力を発揮できる。

> 十分な数のニューロンを持つ隠れ層が少なくとも1つあれば、ニューラルネットワークが「万能関数近似器」になることが実証されている。つまり、有限区間において任意の連続関数を任意の制度で近似できる。

### 7.1.4 逆伝搬によるニューラルネットワークの訓練

　ネットワークトポロジ自体は何も学習していないまっさらな状態であり、新生児と同じように経験を通じた訓練が必要である。ニューラルネットワークが入力データを処理していくうちに、ニューロン間の結合が強くなったり弱くなったりする。赤ちゃんの脳が環境を経験しながら発達していくようなものである。ニューラルネットワークの結合の重みは、観測されたパターンを反映するように調整されていく。\
　結合の重みを調整しながらニューラルネットワークを訓練するときの計算量は膨大である。ANNは数十年前から研究されているが、現実の学習タスクに応用されるようになったのは、ANNを効率よく訓練する手法が発見された1980年代の中頃からである。この、「誤差の逆伝搬」という手法を用いるアルゴリズムは、がんに**誤差逆伝搬**または**バックプロパゲーション**（backpropagation）と呼ばれるようになった。

　他の機械学習アルゴリズムに比べて計算量が少し多いものの、誤差逆伝搬はANNへの関心っを呼び覚ますきっかけとなった。この結果、デーtマイニング分野では、誤差逆伝搬法を用いる多層フィードフォワードネットワークが良く使われるようになっている。このようなモデルには次に示すような長所と短所がある。

+------------------------------------------------------------------------+----------------------------------------------------------------------------------------+
| 長所                                                                   | 短所                                                                                   |
+========================================================================+========================================================================================+
| -   分類問題や数値予測問題に応用できる                                 | -   ネットワークトポロジが複雑な場合は特にそうだが、計算量が膨大で、訓練に時間がかかる |
| -   他のほぼどのようなアルゴリズムよりも複雑なパターンをモデル化できる | -   訓練データの過学習に陥りやすい                                                     |
| -   データの関係について仮想をほとんどおかない                         | -   複雑なブラックボックスモデルになるため、解釈するのが（不可能でないにしても）難しい |
+------------------------------------------------------------------------+----------------------------------------------------------------------------------------+

　最も一般的な形式の誤差逆伝搬法は、２つのプロセスからなるサイクルを何度も繰り返す。各サイクルは**エポック**（epoch）と呼ばれる。ネットワークには先験的な（既存の）知識が全く含まれていないため、最初の重みはたいていランダムに設定される。続いて、アルゴリズムは終了条件が満たされるまでこれらのプロセスを繰り返し実行する。誤差逆伝搬法のエポックは次の2つである。

-   **順伝搬**\
    入力層から出力層までのシーケンスでニューロンを活性化し、その過程で各ニューロンの重みと活性化関数を適用する。最終そうに到達したら、そこで出力信号を生成する。
-   **逆伝搬**\
    順伝搬で生成された出力信号を訓練データの目的変数（正解値）と比較する。ニューロン間の結合の重みを調整し、将来の誤差を減らすために、ネットワークの出力信号と正解値との差を誤差としてネットワークに逆伝搬する。

　誤差伝搬法は、逆伝搬された情報をもとに、ネットワークの全体的あん誤差をへらしていく。しかし、各ニューロンの入ry区と出力の関係が複雑であることを考えると、重みをどれくらい調整すればよいかをアルゴリズムはどのようにして決めるのか、という疑問が残る。その答えは**勾配降下法**（gradient descent）という手法にある。概念的には、ジャングルの中で道に迷った探検家が水場への道を探すの似ている。地形を調べながら最も急な斜面をくだっていけば、やがて最も低い谷間にたどり着く。そこはおそらく川床になっているはずだ。\
　同様に、誤差逆伝搬法も各ニューロンの活性化関数の積分を使って重みごとにその無機の勾配を求める。活性化関数が微分可能であることが重要なのは、そのためである。勾配は重みの変更によって誤差がどれくらい減少または増加するのかを表す。勾配降下法は、**学習率**（learning rate）という値に基づき、誤差の減少が最大になるように重みを変更する。学習率を高くするほど、勾配を素早く降下するようになり、谷を通り過ぎるリスクと引き換えに訓練時間を短縮できる可能性がある。\
　このプロセスは複雑に思えるが、実際には簡単に適用できる。さっそく多層フィードフォワードネットワークに関する知性を現実の問題に対応してみよう。

## 7.2 例：人工ニューラルネットワークを使ってコンクリートの強度をモデル化する

　工学の分野では、建設素材の性能を正確に推定することが非常に重要となる。建物、橋、車道の建設に使われる素材の安全性のガイドラインを定めるには、このような見積もりが不可欠である。\
　コンクリートの強度の見積もりは、特に興味深い課題である。コンクリートはほとんどの建設プロジェクトで使われるが、コンクリートの性能には大きなばらつきがある。原因の体部分は、複雑な相互作用を伴うさまざまな原料にある。結果として、最終的な製品の強度を正確に予測するのは難しい。原料の配合リストからコンクリートの強度を各2つに予測できるモデルがあれば、より安全な建設施工につながることが考えられる。

### ステップ１：データを収集する

　この分析では、コンクリートの圧縮強度に関するデータを集めた Concrete Compressive Strength データセットを使う。このデータセットは1,030個のコンクリートサンプルを含んでおり、サンプルはそれぞれ配合された原料を表す8つの特徴量で構成されている。これらの特徴量は最終的な圧縮強度に関連していると考えられており、製品に使われているセメント、スラグ、灰、水、超可塑剤、粗骨材、細骨材の量（それぞれ$kg/m^3$）と効果時間（日数）で構成されている。

```{r}
# ライブラリの読み込み
if(! require(tidyverse)){
  install.packages('tidyberse', quiet = TRUE)
}
library(tidyverse)
```

### ステップ２：データの調査と前処理

```{r}
# データの読み込み
concrete <- 
  read_csv('https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(3rd%20Ed.)/Chapter07/concrete.csv')

str(concrete)
```

　concreteデータフレームに含まれている9つの変数は、8つの特徴量と1つの目的変数に対応している。しかし、どうやら問題があるようだ。ニューラルネットワークが最もうまくいくのは、入力データが0から1,000を超えるものまで広い範囲に及んでいる。\
　この問題を解決する一般的な方法は、正規化関数または標準化関数を使ってデータの尺度を取り直すことである。データが釣り鐘曲線を描いている場合は、Rの組み込み関数scaleを使って標準化するのが得策かもしれない。一方で、データの範囲が一様分布に従っているなどお、正規分布から大きくかけ離れている場合には0から1の範囲に正規化するほうが適切かもしれない。ここでは正規化を行う。

```{r}
# 標準化関数

normalize <- function(x){
  return((x - min(x)) / (max(x) - min(x)))
}
```

　このコードを実行したあとは、lapply関数を使って、concreteデータクレームの全ての列にnormalize関数を適用できる。

```{r}
# 正規化の実行

concrete_norm <- as_tibble(lapply(concrete, normalize))
```

　正規化がうまく行ったかを確認する。

```{r}
cat('元のデータの要約統計量\n')
print(summary(concrete$strength))
cat('\n')
cat('変換後のデータの要約統計量\n')
print(summary(concrete_norm$strength))
```

> モデルを訓練する前にデータに変換を適用した場合、元の計測値に戻すには、あとで逆の変換を適用しなければならない。尺度の取り直しを用意にするために、元のデータを保存しておくか、少なくとも元のデータの要約統計量を保存しておく必要がある。

　訓練データとテストデータに分割する。

```{r}
# データセットの分割

concrete_train <- concrete_norm[1: 773, ]
concrete_test <- concrete_norm[774: 1030, ]
```

### ステップ３：モデルを訓練する

　コンクリートの原料と完成した製品の強度との関係は、多層フィードフォワードニューラルネットワークを使ってモデル化する。neuralnetパッケージは、このようなネットワークの標準の使いやすい実装を提供する。このパッケージには、ネットワークトポロジをプロットする関数も含まれている。これらの理由により、neuralnetパッケージの実装はニューラルネットワークを学ぶのにもっていこいだが、だからといって実際の作業に使えないわけではない。

> 人工ニューラルネットワーク（ANN）の訓練によくつかわれているパッケージは他にもあるが、それぞれに長所と短所がある。おそらく最もよく引き合いに出されるのは、Rの標準ディストリビューションに統合されているnnetパッケージだろう。このパッケージは標準的な誤差逆伝搬法よりも少し高度なアルゴリズムを使っている。もう1つの選択しはRSNNSパッケージであり、ニューラルネットワークの機能をひととおり備えているが、理解するのが難しいという欠点がある。

　まず、デフォルト設定を使って、隠れノードが1つだけの最も単純な多層フィードフォワードニューラルネットワークを訓練する。

```{r}
# ライブラリの読み込み

if(! require(neuralnet)){
  install.packages('neuralnet', quiet = TRUE)
}
library(neuralnet)
```

```{r}

# コンクリートモデルの訓練
concrete_model <- neuralnet(strength ~ cement + slag + ash + water +
                              superplastic + coarseagg + fineagg + age,
                            data = concrete_train)

# モデルの可視化
plot(concrete_model)
```

　このモデルでは、8つの特徴量のそれぞれに対して入力ノードが1つ存在し、その後に隠れノードが1つ、そしてコンクリートの強度を予測する出力ノードが1つ存在している。各結合の重みに加えて、数字の１で表示されている**バイアス項**（bias term）も含まれている。バイアス項は、線形方程式の切片と同じように、指定されているノードの値を上下にどのくらいシフトできるかを示す数値定数である。

> 隠れノードが1つだけのニューラルネットワークは、線形回帰モデルの一種として考えることができる。各入力ノードと隠れノードの間の重みは回帰係数にいており、バイアス項の重みは切片に似ている。

　上記可視化では、訓練ステップの個数と**誤差平方和**（SSE）で表された誤差も表示されている。SSEは予測値と正解値の差を二乗して合計した値である。SSEの値が小さいほど、もでるは訓練データによく適合している。ただし、これはあくまでも訓練データの性能であり、未知のデータでの性能についてはほとんどわからない。

### ステップ４：モデルの性能を評価する

　テストデータセットで予測値を生成するには、compute関数を使う。

```{r}
model_results <- compute(concrete_model, concrete_test[1:8])
```

　これまで使ってきたpredict関数とは少し勝手が異なり、compute関数は$neuronsと$net.resultの2つの要素からなるリストを返す。$neuronsはネットワークの各層のニューロンを格納し、$net.resultは予測値を格納する。

```{r}
predicted_strength <- model_results$net.result
```

　ここで扱っているのは分類問題ではなく数値予測問題であるため、モデルの正解率を混同行列で調べることはできない。そこで、コンクリートの強度に関する予測値と正解値の相関を求めることにする。予測値と正解値の間に強い相関が認められるとしたら、このモデルはコンクリートの強度に関する有益な指標となる可能性がある。\
　2つの数値ベクトルの相関を求めるには、cor関数を使うことを思い出そう。

```{r}
cor(predicted_strength, concrete_test$strength)
```

> ニューネットワークでは最初の重みがランダムに設定されるため、予測値はモデルごとに異なる可能性がある。これらの結果を正確に一致させたい場合は、ニューラルネットワークを構築する前にset.seed(12345)を実行する

　相関が１に近い場合は、２つの変数の間に強い線形関係があることを意味する。つまり、0.806という相関はかなり強い関係があることを示唆している。このことは、隠れノードが１つだけであっても、このモデルの性能がかなり良いことを示している。\
　このモデルの隠れノードはまだ１つだけなので、その性能を改善できる可能性は高い

### ステップ５：モデルの性能を向上させる

　トポロジが複雑であるほど、そのネットワークはより複雑な概念を学習できる。そこで、隠れノードの個数を５に増やしたらどうなるかを見てみよう。前回と同様にneuralnet関数を使うが、今回はhidden = 5パラメータを追加する。

```{r}
concrete_model2 <- neuralnet(strength ~ cement + slag + ash + water +
                               superplastic + coarseagg + fineagg + age,
                             data = concrete_train, hidden = 5)
```

　ネットワークを再びプロットしてみる。

```{r}
plot(concrete_model2)
```

　表示されている誤差（SSE）が5.08から1.66に減っていることがわかる。それに加えて、訓練ステップの数が7,779から28,376に増えている。モデルがどれくらい複雑になったかを考えれば意外なことではない。もっと複雑なネットワークでは、最適な重みを見つけ出すためにイテレーションの回数がさらに増えることになる。\
　同じ要領で予測値と正解値を比較すると、相関が約0.931であることがわかる。隠れノードが１つだけのときは0.806だったことを考えると、大きな改善である。

```{r}
model_results2 <- compute(concrete_model2, concrete_test[1:8])
predicted_strength2 <- model_results2$net.result
cor(predicted_strength2, concrete_test$strength)
```

　これらは大きな改善だが、モデルの性能を向上させるためにできることがまだ残っている。具体的には、隠れ層をさらに追加することと、ネットワークの活性化関数を変更することが可能である。これらの変更を加えることで、非常に単純なディープニューラルネットワークの原形が得られる。\
　ディープラーニングにとって、活性化関数の選択は非常に重要である。特定の学習タスクに最適な活性化関数は、通常は実験によって特定され、機械学習の研究コミュニティの中で広く共有される。\
　最近では、**正規化線形関数**（recified linear functionまたは単にrecifire）と呼ばれる活性化関数の人気が高い。というのも、あg像認識などの複雑なタスクで実績を上げているからだ。正規化線形関数を使っているニューラルネットワークのノードは**ReLU**（rectified linear unit）と呼ばれる。正規化線形関数は$x$が0以上の場合に$x$を返し、そうでない場合に0を返す関数として定義される。この関数が重要なのは、非線形関数でありながら数学的性質が単純で、計算量が少なく、勾配降下が非常に効率的だからである。残念ながら、この関数の積分は $x=0$で未定義となるため、neuralnet関数では利用できない。\
　代わりに、ReLUを平滑近似したSmoothReLUを使うという方法がある。SmoothReLUは$\log(1+e^x)$で定義される活性化関数であり、**ソフトプラス**（softplus）とも呼ばれる。ソフトプラス関数は$x$が0未満の時は0に近い値となり、$x$が0よりも大きいときは$x$に近い値となる。

```{r}
# ソフトプラス関数の定義

softplus <- function(x){log(1 + exp(x))}
```

　この活性化関数は、 act.fct パラメータを使ってneuralnet関数に渡すことができるう。それに加えて、５つのノードで構成された2つ目の隠れ層を追加する。この隠れ層を追加するには、hiddenパラメータに整数ベクトルc(5, 5)を指定する。

```{r}
set.seed(12345)
concrete_model3 <- neuralnet(strength ~ cement + slag + ash + water +
                               superplastic + coarseagg + fineagg + age,
                             data = concrete_train, hidden = c(5, 5),
                             act.fct = softplus)
```

　ネットワークの可視化は次の通り。

```{r}
plot(concrete_model3)
```

　これでコンクリートの強度に関する予測値と正解値の相関を計算できる。

```{r}
model_results3 <- compute(concrete_model3, concrete_test[1:8])
predicted_strength3 <- model_results3$net.result
cor(predicted_strength3, concrete_test$strength)
```

　予測値と正解値の相関は0.935であり、性能はこれまでで最も良い。興味深いことに、Yehが論文で報告していた相関は0.885だった。つまり比較的わずかな労力で、この分野の専門家に匹敵するかそれ以上の性能を達成できたことになる。もちろん、Yehの数字は1998年に発表されたもので、私達はそれから20年以上にわたってニューラルネットワーク研究の恩恵を受けている。\
　ここで注意しなければならない重要な点が１つある。モデルを訓練する前にデータを正規化したので、これらの予測値の尺度も０から１に正規化されていることである。たとえば、次のデータフレームあh,元のデータセットの今クロート強度値をそれらに対応する予測値と比較している。

```{r}
strengths <- data.frame(actual = concrete$strength[774:1030],
                        pred = predicted_strength3)
head(strengths, n = 3)
```

　相関を調べてみると、データを正規化するかどうかの選択が性能指標の計算に影響を与えないことがわかる。0.935という相関は先ほどと全く同じである。

```{r}
cor(strengths$pred, strengths$actual)
```

　しかし、予測値と正解値の絶対差など、別の性能指標を計算する場合は、尺度の選択が非常に必要となる。\
　このことを念頭に置いて、unnormalize関数を作成する。今kなん数はmin-max正規化とは逆の手続きをおこない、正規化した予測値を元の削除に変換できるようにする。

```{r}
unnormalize <- function(x){
  return((x * (max(concrete$strength)) - min(concrete$strength)) +
           min(concrete$strength))
}
```

　このカスタム関数unnormalizeを予測値に適用すると、変換後の予測値の尺度が元のコンクリート強度値のものと同じになる。これなら、意味のあるご差の絶対値を計算できる。これに加えて、変換後の予測値元の強度値の相関も同じままである。

```{r}
strengths$pred_new <- unnormalize(strengths$pred)
strengths$error <- strengths$pred_new - strengths$actual
head(strengths, n = 3)
```

　ニューラルネットワークをプロジェクトに利用するときには、同様の手続きをおこなってデータを元の尺度に戻す必要がある。\
　また、もっと難易度の高い学習タスクでは、ニューラルネットワークがすぐに複雑になってしまうこともわかるだろう。たとえば、いわゆる**勾配消失問題**（vanishing gradient problem）や**勾配発散問題**（exploading gradient problem）に直面することがあるかもしれない。これらは誤差逆伝搬法が妥当な時間内に収束できないために有益な解を見つけ出せないという問題である。打開策としては、隠れノードの個数を変える、ReLUなど別の活性化関数を使う、学習率を調整するなどが考えられる。?nerralnetコマンドを使ってヘルプファイルを表示すれば、調整可能なさまざまなパラメータに関する情報が見つかる。しかし、テストするパラメータの数が多ければ多いで、高性能なモデルの構築の妨げになる。このことはANNのトレードオフであり、DNNではさらにやっかいな問題となる。ANNやDNNの潜在能力を引き出すには、時間と計算能力への多大な投資が求められる。

## 7.3 サポートベクトルマシンを理解する

　**サポートベクトルマシン**（support vector machine :SVM）については、多次元空間にプロットされたデータ点が境界面で区切られていると考えることができる。Sこの多次元空間はインスタンスとその特徴量の値を表している。SVMの目標は、**超平面**（hyperplane）と呼ばれる境界面を作成することである。超平面で区切られた空間の両側には、ほぼ同質のグループがそれぞれ作成される。このため、SVMは、インスタンスベースの最近近傍法学習器と線形回帰モデルの両方の性質を組み合わせたのになる。この組合わせは、SVMが非常に複雑な関係をモデル化できるほど強力である。\
　SVMのもとになっている基本的な数学は数十年前から知られているものだが、興味深いことに、SVMがおおきく成長したのは機械学習コミュニティに受け入れられた後のことである。\
　SVMを実装するのに必要な数学は少し複雑で、誰もが操れるようなものではなかったが、さまざまなプログラミング言語のライブラリとして実装されたことにより、SVMは幅広いユーザ尾を獲得した。数学は難しいかもしれないが、基本的な概念が理解できるものであることも有利に働いている。\
　SVMは分類問題と数値予測問題を含め、ほぼあらゆる種類のタスクに適応できる。このアルゴリズムの重要な成功の多くは、パンターン認識に関連している。次に、注目すべき応用例をあげておく。

-   バイオインフォマティクス分野でのマイクロアレイ遺伝子発現データの分類によるがんなどの遺伝子異常の識別
-   文章で使われている言語の識別や主題による文書の分類といったテキスト分類
-   燃焼機関の欠陥、セキュリティ侵害、地震など、非常に稀ながら重大な事象の検出

　SVMが最も理解しやすいのは、二値分類に使ったときである。SVMは伝統的に二値分類に使われてきた。そこで、ここではSVM分類器に焦点を合わせる。ここで説明する原則は、SVMを数値予測に適用するときにも当てはまる。

### 7.3.1 超平面による分類

　SVMは超平面と呼ばれる境界を使って同じようなクラス値を持つデータをグループに分割する。直線または平面によって完全に分割できるデータは**線形分離可能**（linearly separable）と呼ばれる。まず最初に、線形分離可能な単純なケースだけを取り上げ、その後SVMを拡張子、データが線形分離可能ではない問題についても取り上げる。\
　２次元でのSVMのタスクは、2つのクラスを分割する直線を見つけ出すことである。データによりそのような直線がいくつも引ける場合がある。その場合SVMはどのように直線を選ぶのだろうか。\
　その質問に対する答えは、**マージン最大化超平面**（maximum margin hyperplane : MMH）を探すことである。MMHは２つのクラスからの距離が最大となる超平面である。既知であるデータでうまく分離できる直線はいくつも存在するが、未知のデータに最もうまく汎化するのは、２つのくらすからの距離が最大となる直線だろう。マージンの最大化は、ランダムノイズが追加されたとしても、各クラスが境界の適切な側に留まる可能性を向上させる。\
　各クラスのデータ点のうちMMHに最も近い点のことを**サポートベクトル**（supoort vector）と呼ぶ。各クラスには少なくともサポートベクトルが１つは存在する。また、複数になることもある。MMHはサポートベクトルだけでも定義できる。これはSVMの重要な特徴の１つである。特徴量の個数が極端に多い場合でも、分類モデルを非常にコンパクトに格納する手段がサポートベクトルによって可能となる。

#### 線形分離可能なデータの場合

　最大マージンのt苦丁が最も簡単なのは、クラスが線形分離可能であると仮定したときである。この場合、MMHはデータ点からなる２つのグループの外部境界から可能な限り離れている。このような外部境界を**凸包**（convex hull）と呼ぶ。このとき、MMHは２つの凸包間の最短距離に対する垂直二等分線である。**二次最適化**（quadratic optimization）と呼ばれる手法を用いる高度なコンピュータアルゴリズムは、最大マージンをこのようにして特定できる。\
　この方法に相当するもう１つのアプローチは、考えられるすべての超平面からなる空間を検索することで、データ点を同種のグループに分割する２つの平行な超平面のうち、可能な限り離れているペアを特定するというものである。たとえるなら、寝室への階段の吹き抜けにかからない最も分厚いマットレスを探すようなものだ。\
　この検索プロセスを理解するには、超平面の意味を正確に定義する必要がある。ｎ次元空間では、次の式を使う。

$$
\vec{w}\cdot\vec{x}+b=0
$$

　文字の上についている矢印は、それらが１つの数値ではなくベクトルであることを示す。具体的に言うと、$w$は$n$個の重みからなるベクトル$\{w_1,w_2,\ldots,w_n\}$である。$b$はバイアスと呼ばれる単一の数値である。概念的には、バイアスは傾き切片形の切片項に相当する。\
　このプロセスの目標は、この式を使って、２つの超平面を定義する重みベクトルを特定することにある。

$$
\vec{w}\cdot\vec{x}+b\geq+1\\
\vec{w}\cdot\vec{x}+b\leq-1
$$

　また、これら２つの超平面を、一方のクラスのデータ点がすべて１つ目の超平面の上に配置され、もう一方のグラスのデータ点がすべて２つ目の超平面の下に配置されるように定義する必要もある。データが線形分離可能である限り、このような定義は可能である。\
　ベクトル幾何学では、これら２つの超平面の距離を次のように定義している。

$$
\frac{2}{\lVert\vec{w}\rVert}
$$

　ここで、$\lVert\vec{w}\rVert$は**ユーグリッドノルム**（Euclidean norm）を表す。ユーグリッドノルムは原点からベクトル$w$までの距離である。$\lVert\vec{w}\rVert$は分母にあるため、距離を最大化するには、$\lVert\vec{w}\rVert$を最小化しなければならない。このタスクを一連の制約として表すと、次のようになる。

$$
min\frac{1}{2}\lVert\vec{w}\rVert^2\\
\boldsymbol{s.t.}\ \ y_i(\vec{w}\cdot\vec{x_i}-b)\geq1,\forall\vec{x_i}
$$

　ややこしく思えるかもしれないが、概念として理解する分にはそれほど複雑ではない。各業の基本t機な意味は次のとおりである。１行目は、ユーグリッドノルムを最小化する必要があることを示している（二乗して２で割っているのは計算を容易にするためである。２行目は、このタスクが「データ点$y_i$がそれぞれ正しく分類される」という条件の対照（$s.t.$)になることを示している。yがクラス値（＋１か－１にへんかんされる）を表すことと、$\forall$がすべての～について」を表すことに注意しよう。\
　最大マージンを特定するもう１つの手法と同様に、この問題への解を見つけ出す作業は二次最適化ソフトウェアに任せるのが得策である。プロセッサ次第かもしれないが、データセットがかなり大きい場合でも、専用のアルゴリズムなら凝っレラの問題を素早く解くことができる。

#### 線形分離不可能なデータの場合

　データが線形分離可能ではない場合はどうなるのだろう。この問題は**スラック変数**（skack variable）を使って解決できる。スラック変数はソフトマージンを作成する。ソフトマージンでは、いくつかのデータ点が超平面の誤った側にある２つのデータ点と、それらにたいおうするスラック項（$\xi$）を示している。\
　コスト値（$C$）は、線形制約に違反するすべてのデータ点に適用される。そして、アルゴリズムは最大マージンを見つけ出すのではなく、コストの合計値を最小化することを試みる。したがって、最適化問題を次のように書き直すことができる。

$$
min\frac{1}{2}\lVert\vec{w}\rVert^2+C\sum_{i=1}^{n}{\xi_i}\\
\boldsymbol{s.t.}\ \ y_i(\vec{w}\cdot\vec{x_i}-b)\geq1-\xi_i,\forall\vec{x_i},\xi_i\geq0
$$

　よくわからなくても、心配はいらない。技術的な詳細を理解していなくても、この部分はSVMのパッケージが自動的に最適化してくれる。ここで重要になるのは、コストパパラメータ$C$が追加されていることである。この値を変更すると、超平面の誤った側にあるデータ点に対するペナルティが調整される。コストパラメータの値が大きいほど、最適化が100%の分離を達成す事にこだわるようになる。対照的に、コストパラメータの値が小さいほど、全体的なマージンを大きくすることのほうが重要視される。未知のデータにうまく汎化するモデルを構築するには、これら２つのバランスを取ることが重要になる。

### 7.3.2 非線形空間でのカーネルの使用

　多くの現実のデータセットでは、変数間の関係は非線形である。スラック変数を追加すると制約が緩和され、一部のデータ点の誤分類が認められるよになるため、そのようなデータでもSVMを訓練できる。しかし、非線形問題に対するアプローチはそれだけではない。SVMには、**カーネルトリック**（kernel trick）と呼ばれるプロセスを使っt問題を高次元空間に写像できる重要な特徴がある。そのようにすると、非線形の関係が突如として線形の関係に見えてくることがある。\
　非線形カーネルを用いるSVMは、データに新たな次元を追加することで線形分離を可能にする。基本的には、カーネルトリックは新しい特徴量を生成するプロセスである。この新しい特徴量は、観測された特料長官の数学的な関係を表現するものとなる。\
　たとえば、高度特徴量は緯度と軽dの相互作用として数学的に表現できる。データ点がこれらの尺度の中心に近づくほど、高度が高くなる。このようにすると、本のデータでは明示的に数量化されていなかった概念をSVMが学習できるようになる。\
　非線形カーネルを用いるSVMは以上に強力な分類機だが、次に示すように弱点もある。

+-------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+
| 長所                                                                                                                    | 短所                                                                                                         |
+:========================================================================================================================+:=============================================================================================================+
| -   分類問題者数値予測問題にも利用できる                                                                                | -   最適なモデルを見つけ出すには、カーネルとモデルパラメータのさまざまな組み合わせをテストする必要がある     |
| -   データのノイズの影響をそれほど受けず、過学習に陥りにくい                                                            | -   訓練に時間がかかることがある。入力データセットの特徴量やインスタンスの個数が多い場合には特に時間がかかる |
| -   良くサポートされているSVMアルゴリズムがいくつか存在することもあり、ニューラルネットワークよりも使いやすいことがある | -   複雑なブラックボックスモデルとなるため、解釈するのが（不可能ではないにせよ）難しい                       |
| -   正解率の高さとデータマイニングコンテストでの輝かしい功績により、人気を博している                                    |                                                                                                              |
+-------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+

　一般に、カーネル関数は次のような形式を取る。別の空間へのデータ写像は、関数$\phi(x)$で表せれる。したがって、一般的なカーネル関数は、何らかの変換を特徴量ベクトル$x_i$と$x_j$に適用し、**ドット積**（dot product）を使ってそれらを組み合わせる。つあｍる、２つのベクトルをとり、１つの数値を返す。

$$
K(\vec{x_i},\vec{x_j})=\phi(\vec{x_i})\cdot\phi(\vec{x_j})
$$

　この形式をもとに、さまざまな問題領域でカーネル関数が開発されてきた。次に、最もよく好か割れているカーネル関数をいくつか紹介する。これらを始めとするカーネル関数は、ほぼすべてのSVMパッケージに含まれている。\
　**線形カーネル**（linear kernel）は、データを全く変換しない。したがって、特徴量のドット積として単純に表現できる。

$$
K(\vec{x_i},\vec{x_j})=\vec{x_i}\cdot\vec{x_j}
$$

　次数$d$の**多項式カーネル**（polynomial kernel）は、データの単純な非線形変換を追加する。

$$
K(\vec{x_i},\vec{x_j})=(\vec{x_i}\cdot\vec{x_j}+1)^d
$$

　**シグモイドカーネル**（sigmoid kernel）は、によって得られるSVMもでるは、シグモイド活性化関数を使うニューラルネットワークに少し似ている。カーネルパレメータとしてギリシャ文字$\kappa$と$\delta$をつかう。

$$
K(\vec{x_i},\vec{x_j})=tanh(\kappa\vec{x_i}\cdot\vec{x_j}-\delta)
$$

　**ガウスRBFカーネル**（Gaussian RBF kernel）は、RBFニューラルネットワークに似ている。様々な種類のデータで良い性能が得られるRBFカーネルは、大奥の学習タスクにとって妥当な出発点と考えられている。

$$
K(\vec{x_i},\vec{x_j})=e^\frac{-\lVert\vec{x_i}-\vec{x_j}\rVert^2}{2\delta^2}
$$ 　特定の学習タスクとカーネルを組み合わせるための確かな法則は存在しない。カーネルと学習タスクの相性は、訓練データの量と特徴量間の関係に加えて、学習しようとしている概念にもおおきく左右される。多くの場合は、何種類かのSVMを選択し、SVMを訓練しては検証データセットで評価するという試行錯誤が求められる。とはいえ、性能にそれほど差はないため、カーネルの選択はたいてい恣意的に行われる。SVM分類器に関する知識を現実の問題に応用し、実際にどうなるのか見てみよう。

## 7.4 例：SVMを使って文字を認識する

　多くの機械学習アルゴリズムにとって、画像処理は難しいタスクである。ピクセルパターンをより高度な概念に結びつける関係は非常に複雑で、定義するのが難しい。たとえば、人園にとって顔、ネコ、文字Ａを認識するのは簡単だが、これらのパターンを厳格なルールとして定義することは難しい。さらに、画像データにはノイズが混じっていることがよくある。対象に当たる光の加減や対照の向きや位置を変えれば、ほんの少し異なる画像をいくらでもキャプチャーできる。\
　ＳＶＭは画像データという難題に取り組むのに非常に適している。ノイズに過剰に反応することなく複雑なパターンを学習できるため、高い正解率でビジュアルパターンを認識できる。しかもＳＶＭの主な弱点である「ブラックボックスモデル表現」は、画像処理ではそれほど重大でない。ＳＶＭがイヌとネコを見分けられるのなら、どのようにして見分けるのかは大きな問題ではない。\
　ここでは、ＯＣＲ（Optical Character Recognition）ソフトウェアで使われているようなモデルを開発する。ＯＣＲは、よくデスクトップ型のドキュメントスキャナーにバンドルされていたり、スマートフォンアプリで使われていたりする。ＯＣＲの目的は、紙媒体の文章を処理することであり、印刷された文字や高木の文字を電子的な形式に変更してデータベースに保存で切るようにする。もちろん、毛質や印刷に使われるフォントはさまざまであるため、これは難しい問題である。たとえそうであっても、ビジネス環境において誤字脱字は恥ずかしい誤りであり、高く付くことがあるため、ＯＣＲのユーザは完璧を求める。ＳＶＭがこのタスクに向いているかどうかを見てみよう。

### ステップ１：データを収集する

　ＯＣＲソフトウェアが文章を処理するときには、まず、紙をグリッド上に分割し、グリッドの各セルに**グリフ**（glyph）が１つだけ含まれるようにする。グリフは、文字、記号、数字を表す用語である。次に、それらのセルごとに、そのグリフをソフトウェアが認識しているすべての文字の集合と照合する。最後に、個々の文字をつないで単語にする。その際には、文章に使われている言語の辞書でスペルチェックを行うこともある。\
　この例では、文章を分割して文字が矩形領域に１つずつ含まれた状態にするアルゴリズムはすでに開発済みであると仮定する。また、文章に含まれているのは英語のアルファベット文字だけである。したがって、Ａ～Ｚの２６文字のいずれかとグリフを照合するプロセスをシミュレートすることになる。\
　この作業には、Letter Recognitionデータセットを使う。このデータセットは２６の大文字のアルファベットからなる20,000個のデータを含んでいる。これらのデータは２０種類の白黒フォントでインs夏されており、それらのフォントの形状はランダムに歪められている。

```{r}
# データの読み込み

letters <- read_csv('https://raw.githubusercontent.com/dataspelunking/MLwR/master/Machine%20Learning%20with%20R%20(3rd%20Ed.)/Chapter07/letterdata.csv')
```

### ステップ２：データの調査と前処理

　このデータのグリフはピクセルに変換されており、１６の統計量が属性として記録されている。\
　これらの属性は、グリフの縦横の大きさ、黒ピクセルの（白に対する）割合、ピクセルの縦横の位置の平均などを示している。おそらく、グリッド内での黒ピクセルの集中度の違いが、アルファベッドの２６文字を区別する手段になるはずだ。

```{r}

# lettersが文字型なので、因子型に変換する
letters <- mutate(letters, across(where(is.character), as.factor))

# データの確認
str(letters)
```

　ＳＶＭ学習器は、すべての学習量が数値であることと、各特徴量が比較的狭い範囲にスケーリングされていることを要求する。この場合、各特徴量は整数であるため、因子を数値に変換する必要はない。一方で、特徴量によっては、整数の範囲がかなり広いものがあるようだ。このことは、データの正規化または標準化が必要であることを示唆している。しかし、ＳＶＭモデルの訓練に用いるＲパッケージはスケーリングを自動的に行うため、この手順は省略できる。\
　でt-亜の前処理を行う必要がないため、早速機械学習プロセスの訓練フェーズとテストフェーズに取り掛かることができる。以前の分析では、データを訓練データセットとテストデータセットにランダムに分割した。kの分析でもそのようにすることは可能だが、このデータはすでにデータをランダム化しており、最初の16,000個（80％）のデータをモデルの構築に使い、残りの4,000個（20%）のデータをテストに使うことを前提としている。

```{r}
letters_train <- letters[1:16000, ]
letters_test <- letters[16001:20000, ]
```

### ステップ３：モデルを訓練する

　Ｒには傑出したＳＶＭパッケージがいくつかあり、ＳＶＭモデルをＲで訓練するときには、その中からどれかを選択できる。ウィーン工科大学の統計学科が開発しｅ１０７１パッケージは、Ｃ＋＋で書かれたＬＩＢＳＶＭライブラリへのＲインタフェースを提供する。ＬＩＢＳＶＭは広く使われているオープンソースのＳＶＭプログラムであり、賞を獲得している。LIBSVMをすでによく知っている場合は、　ｅ1071パッケージを使うとよい。\
　同様に、SVMlightアルゴリズムを経験している場合は、ドルトムント工科大学の統計学科が開発したklaRパッケージを使うという手もある。このパッケージは、このＳＶＭ実装をＲから直接操作するための関数を提供する。\
　最後に、ゼロから始める場合は、kernlabパッケージのＳＶＭ関数がおそらく最適である。このパッケージはCやC++ではなくＲでネイティブに開発されているため、簡単にカスタマイズできるという興味深い利点を持つ。舞台裏に隠された仕掛けはまったくない。おそらくそれよりも需要なのは、他の選択肢とは異なり、kernlabパッケージをcaretパッケージと併用できることだろう、caretパッケージを利用すれば、自動化されたさまざまな手法を使ってＳＶＭモデルの訓練と評価を行うことができる。\
　kernlabパッケージを使ってSVM分類器を訓練するための構文は次のとおりである。他のパッケージを使うことにした場合でも、コマンドはだいたい同じである。ksvm関数はデフォルトでガウスRBFカーネルを使うが、他のオプションもサポートされている。\
　SVMの性能指標のベースラインを設定するために、最初は簡単な線形SVM分類器を訓練する。

```{r}
if(! require(kernlab)){
  install.packages('kernlab', quiet = TRUE)
}

library(kernlab)
```

　ksvm関数を呼び出し、訓練データセットとkernel = 'vanilladot' を指定する。

```{r}
letter_classifier <- ksvm(letter ~ ., data = letters_train,
                          kernel = 'vanilladot')
```

　モデルの訓練パラメータと適合度に関する基本情報を確認する。

```{r}
letter_classifier
```

　テストデータで性能を評価する

### ステップ４：モデルの性能を評価する

　predict関数を利用すれば、テストデータセットでSVM分類器に予測値を生成させることができる。

```{r}
letter_predictions <- predict(letter_classifier, letters_test)
```

　typeパラメータを指定しなかったので、デフォルトの type = 'response' が使われる。このため、 テストデータの行ごとに予測された文字が含まれたベクトルが返される。head関数を呼び出すと、 最初の６つの文字がU、N、V、X、N、Hであることがわかる。

```{r}
head(letter_predictions)
```

　この分類器の性能を調べるには、予測値（予測された文字）をテストデータセットの正解値 （実際の文字）と比較する必要がある。この作業にはtable関数を使うことにしよう。

```{r}
table(letter_predictions, letters_test$letter)
```

　対角線上に並んでいる114、121、120、156、127という数字は、予測値と正解地が一致した データの合計数を示している。同様に、一致しなかったデータの合計値も示されている。たとえば、 Ｂ行Ｄ列の5という数字は、文字Dが文字Bと誤認識されたケースが５つあることを示している。\
　これらの誤分類を其々調べてみると、到底の種類の文字を識別するのにモデルが苦労しているという パターンが見えてくるかもしれない。しかし、余計な時間がかかるため、 興味深いなどど言っている場合ではない。そこで、評価を単純にするために、全体的な正解率 だけを計算する。ここでは、予測値が正しいかどうかだけを考慮し、誤分類の種類は無視する。\
　次のコマンドは、モデルの予測値がデータセットの正解値と一致したかどうかを示す TRUE または FALSE の値からなるベクトルを返す。

```{r}
agreement <- letter_predictions == letters_test$letter
```

　table 関数を呼び出すと、このモデルが正しく認識したのは、4,000個のテストデータのうち 3,357個であることがわかる。

```{r}
table(agreement)
```

　正解率は約84%である。

```{r}
table(agreement) %>% prop.table()
```

　Frey と Slate がこのデータセットを1991年に発表したときの認識率が約80%だたことに 注目しよう。たった数行のＲコード２人の記録を塗り替えてしまったが、私達はそれから 20年以上にわたって機械学習研究の恩恵を受けている。そう考えると、モデルの性能をさらに 改善できそうである。

### ステップ５：モデルの性能を向上させる

　ここで、画像データからアルファベッtの文字を認識するために訓練した SVM モデルの 性能について、前後関係を整理してみよう。たった１行のＲコードで、このモデルは84%近い 正解率を達成することができた。この数字は1991年に２人の研究者が発表したベンチマーク を少し上回っている。84%の正解率は、OCR ソフトウェアで利用するのに十分であるとはお世辞 にも言えないが、比較的単純なモデルがこのレベルに達成できたのはそれだけで快挙である。 モデルの予測値がまぐれで正解地と一致する確率はきわめて低く、4%に満たない。つまり、 このモデルの性能は当て推量の20倍以上である。このことから、SVM 関数のパラメータを調整 してもう少し複雑なモデルを訓練すれば、現実でも通用するモデルが見つかる可能性がある。

#### SVMのカーネル関数を変更する

　先のSVM モデルでは、単純な線形カーネル関数を使った。より不k雑なカーネル関数を使って データをくじ現空間に写像すれば、より性能のよいもでるから得られる可能性がある。\
　しかし、さまざまなカーネル関数の中からどれを選ぶかが問題である。常套手段は、さまざ まな種類のデータで良い性能が得られることがわかっているガウスRBFカーネルから始める ことだ。ksvm関数を使ってRBFベースのSVMモデルを訓練してみよう。

```{r}
letter_classifier_rbf <- 
  ksvm(letter ~ ., data = letters_train, kernel = 'rbfdot')
letter_predictions_rbf <- predict(letter_classifier_rbf, letters_test) 
```

　最後に、正解率を線形SVMモデルのものと比較する。

```{r}
agreement_rbf <- letter_predictions_rbf == letters_test$letter
table(agreement_rbf)
```

　正解率を計算すると

```{r}
table(agreement_rbf) %>% prop.table()
```

　カーネルを変更しただけで、文字認識モデルの正解値を84%から93%に引き上げることができた。

#### SVMの最適なコストパラメータを特定する

　このレベルの性能ではまだOCRプログラムにとって十分ではない場合、もちろん、他のカーネル をテストすることが可能である。しかし、もう1つの有益なアプローチとして、コストパラメータ でいろいろな値をs試すことで、SVMの決定境界（超平面）の幅を変えてみるという方法がある。 このようにすると、訓練データに対するモデルの過学習と学習不足の’バランスを制御できる。 コストパタメータの値を大きくすると、誤分類のたびに大きなペナルティが課されるため、 学習器は訓練データを一つ残らず完全に分類しようとする（過学習）。一方で、コスト パラメータの値が小さすぎると、訓練データの重要ながら捉えがたいパターンを学習器が 見逃してしまい、正解パターンをうまく学習d形ないことがある。（学習不足）。\
　理想的な値を事前に知るための経験則は存在しないため、コストパラメータCにさまざまな 値を試しながらモデルの性能を調べることにする。sapply関数を利用すれば、訓練プロセス と評価プロセスを繰り返す代わりに、コスト値からなるベクトルにカスタム関数を適用できる。 まず、seq関数を使って、このベクトルを5から40までの等間隔（増分5）のシーケンスとし て生成する。続いて、これらのコスト値ごとに、カスタム関数を使ってモデルの性能を以前と 同じように行い、テストデータセットで予測値を生成する。そして、各モデルの正解率を、正 解値と一致した予測値の個数と予測値の総数で割った値として求める。最後に、plot関数を 使って結果を可視化する。

```{r}
cost_values <- c(1, seq(from = 5, to = 40, by =5))

accuracy_values <- sapply(cost_values, function(x){
  set.seed(12345)
  m <- ksvm(letter ~ ., data = letters_train, kernel = 'rbfdot', C = x)
  pred <- predict(m, letters_test)
  agree <- ifelse(pred == letters_test$letter, 1, 0)
  accuracy <- sum(agree) / nrow(letters_test)
  return(accuracy)
})

plot(cost_values, accuracy_values, type = 'b')
```

　図に示されているように、デフォルトのコストパラメータ C = 1 のときの正解率は93%で、 ここで評価している９つのモデルの中で最も性能が低い。これに対し、コストパラメータに10 かそれ以上の値を設定すると、正解率は97%近くになり、性能がおおきく改善されたことがわかる。 おそらく、現実の環境にデプロイしても十分なほど完璧なモデルだが、引き続きさまざま なカーネルを試して、100%の正解率に近づけるかどうか調べてみる価値があるだろう。正解 率が改善されるたびに、ＯＣＲソフトウェアの誤分類が少なくなり、エンドユーザの全体的な エクスペリエンスが改善されるからだ。

## 7.5 まとめ

　本章では、大きな可能性を秘めているものの、その複雑さのせいで見過ごされがちな機械 学習の手法を２つ取り上げた。これらの手法に対する評価が不当とは言えないまでも適切ではな いことに気づいてもらえたことを願っている。ANNとSVMの基本概念は非常に簡単に理解で 着るものだ。\
　一方で、ANNとSVMは数十年前から存在しているため、どちらもさまざまな種類に分かれている。 本章では、これらの手法によって何が可能になるかをざっと説明したにすぎない。ここで 覚えた用語は、拡大する一方のディープラーニング分野を含め、日々開発されているさまざまな 技術的進歩の微妙な違いを理解するのに役立つだろう。\
　ここまでは、単純なものから高度なものまで、様々な種類の予測モデルを取り上げてきた。 次章では、他の種類の学習タスクで使われている手法について検討する。教師なし学習と 呼ばれるそれらの手法により、データの中に埋もれている興味深いパターンが明らかになる。
